{"cells":[{"cell_type":"markdown","metadata":{"id":"q2GAY0Pobgaf"},"source":["<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n","\n","<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"]},{"cell_type":"markdown","metadata":{"id":"cfXQ_NuLbgah"},"source":["## Objective\n"]},{"cell_type":"markdown","metadata":{"id":"43mi1vwJbgai"},"source":["In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"]},{"cell_type":"markdown","metadata":{"id":"3139_AYybgai"},"source":["## Table of Contents\n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","\n","<font size = 3>\n","    \n","1. <a href=\"#item31\">Import Libraries and Packages</a>\n","2. <a href=\"#item32\">Download Data</a>  \n","3. <a href=\"#item33\">Define Global Constants</a>  \n","4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n","5. <a href=\"#item35\">Compile and Fit Model</a>\n","\n","</font>\n","    \n","</div>\n"]},{"cell_type":"markdown","metadata":{"id":"BPgX5a3fbgaj"},"source":["   \n"]},{"cell_type":"code","source":["!pip install Keras-Applications\n","!pip install skillsnetwork\n","!pip install tensorflow==2.8.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"As-Y898Ucacu","executionInfo":{"status":"ok","timestamp":1725160099701,"user_tz":-60,"elapsed":9035,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}},"outputId":"1446ab60-d90e-4993-d7e1-fb8191f9c045"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Keras-Applications in /usr/local/lib/python3.10/dist-packages (1.0.8)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from Keras-Applications) (1.26.4)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from Keras-Applications) (3.11.0)\n","Requirement already satisfied: skillsnetwork in /usr/local/lib/python3.10/dist-packages (0.21.9)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from skillsnetwork) (7.34.0)\n","Requirement already satisfied: ipywidgets<9,>=8 in /usr/local/lib/python3.10/dist-packages (from skillsnetwork) (8.1.5)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from skillsnetwork) (2.32.3)\n","Requirement already satisfied: tqdm<5,>=4 in /usr/local/lib/python3.10/dist-packages (from skillsnetwork) (4.66.5)\n","Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=8->skillsnetwork) (0.2.2)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=8->skillsnetwork) (5.7.1)\n","Requirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=8->skillsnetwork) (4.0.13)\n","Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=8->skillsnetwork) (3.0.13)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->skillsnetwork) (71.0.4)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->skillsnetwork) (0.19.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->skillsnetwork) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->skillsnetwork) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->skillsnetwork) (3.0.47)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->skillsnetwork) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->skillsnetwork) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->skillsnetwork) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->skillsnetwork) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->skillsnetwork) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->skillsnetwork) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->skillsnetwork) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->skillsnetwork) (2024.7.4)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->skillsnetwork) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->skillsnetwork) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->skillsnetwork) (0.2.13)\n","Requirement already satisfied: tensorflow==2.8.0 in /usr/local/lib/python3.10/dist-packages (2.8.0)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (24.3.25)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.11.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.1.2)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (18.1.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.26.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.3.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (71.0.4)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0)\n","Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0.dev2021122109)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.37.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.64.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.44.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.7)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.32.3)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.0.4)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2024.7.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n"]}]},{"cell_type":"markdown","metadata":{"id":"6q2Js51Lbgaj"},"source":["<a id='item31'></a>\n"]},{"cell_type":"markdown","metadata":{"id":"X4w86al1bgak"},"source":["## Import Libraries and Packages\n"]},{"cell_type":"markdown","metadata":{"id":"liYIjBR0bgak"},"source":["Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"]},{"cell_type":"code","execution_count":11,"metadata":{"tags":[],"id":"WuoagZEMbgal","executionInfo":{"status":"ok","timestamp":1725160099701,"user_tz":-60,"elapsed":9,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}}},"outputs":[],"source":["import skillsnetwork"]},{"cell_type":"markdown","metadata":{"id":"jt511zuibgam"},"source":["First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"]},{"cell_type":"code","execution_count":12,"metadata":{"tags":[],"id":"1adXs_iobgam","executionInfo":{"status":"ok","timestamp":1725160099702,"user_tz":-60,"elapsed":8,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}}},"outputs":[],"source":["from keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"markdown","metadata":{"id":"ncxYnS5abgan"},"source":["In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"]},{"cell_type":"code","execution_count":13,"metadata":{"tags":[],"id":"T9pzSvStbgao","executionInfo":{"status":"ok","timestamp":1725160099702,"user_tz":-60,"elapsed":8,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}}},"outputs":[],"source":["import keras\n","from keras.models import Sequential\n","from keras.layers import Dense"]},{"cell_type":"markdown","metadata":{"id":"fIWJ6pJcbgap"},"source":["Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"]},{"cell_type":"code","execution_count":14,"metadata":{"tags":[],"id":"pu0xVV6xbgap","executionInfo":{"status":"ok","timestamp":1725160099702,"user_tz":-60,"elapsed":6,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}}},"outputs":[],"source":["from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.applications.resnet50 import preprocess_input"]},{"cell_type":"markdown","metadata":{"id":"CeUi806vbgap"},"source":["<a id='item32'></a>\n"]},{"cell_type":"markdown","metadata":{"id":"8_HolCyIbgaq"},"source":["## Download Data\n"]},{"cell_type":"markdown","metadata":{"id":"MMxaKqaqbgaq"},"source":["In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"]},{"cell_type":"code","execution_count":15,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":35,"referenced_widgets":["716a9ad155a34b919f6be7b2013eb36e","d82595d2c6b8412b8f8fc436a027767c","daa35ae1f892401da529c05d091497ab","72464e2bdf974ff380c666d01c682d4a","c477199dfc2142a1ad214f94d8a23304","8a88156e67bb435d873b8458729d037c","97ceef39cc1e40af9087eeea4cb2730e","d73b246771c7405ba3fa31819815c789","7c141efd6ec449088c17a32eeb3d93c8","da5f6102fae542e8826ac2ad154bf48a","37a2e4704ccc4655bed0f8ce6ffd5b3d","31e8b83893f747b39ae3dfc33c39b44c","7cf2196ca9a34b9ba8a43cd36c59ed97","cc21ae39483b414cbe99b64f42973eed","9099ea20b3b94a36a6aafa492651e6d7","ffcf226cabbc411e911034cff2cadb14","3bf2afd1d0b14cc78430814cea3fe9aa","a40944a791564640b940292edb76af8d","1ae1830af8a14cf89cc1c25d5e8c0067","d90ef681d9fc4a9180797e0c4944a2e4","121a1dbe8a154f06b5dface1827f50c6","78b3fd205ed74a618d7fb95bcc8fa925"]},"id":"7PHP_BnLbgaq","executionInfo":{"status":"ok","timestamp":1725160114371,"user_tz":-60,"elapsed":14675,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}},"outputId":"39533d6f-3060-4266-f478-cc8297070d37"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading concrete_data_week3.zip:   0%|          | 0/97863179 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"716a9ad155a34b919f6be7b2013eb36e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/30036 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31e8b83893f747b39ae3dfc33c39b44c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saved to '.'\n"]}],"source":["## get the data\n","await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\", overwrite=True)"]},{"cell_type":"markdown","metadata":{"id":"PvGIMKxobgar"},"source":["Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"]},{"cell_type":"markdown","metadata":{"id":"QNMeUE4Vbgar"},"source":["**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"]},{"cell_type":"markdown","metadata":{"id":"CJrBAGTtbgar"},"source":["<a id='item33'></a>\n"]},{"cell_type":"markdown","metadata":{"id":"kxR0i3k4bgar"},"source":["## Define Global Constants\n"]},{"cell_type":"markdown","metadata":{"id":"B046LeaJbgar"},"source":["Here, we will define constants that we will be using throughout the rest of the lab.\n","\n","1. We are obviously dealing with two classes, so *num_classes* is 2.\n","2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n","3. We will training and validating the model using batches of 100 images.\n"]},{"cell_type":"code","execution_count":16,"metadata":{"tags":[],"id":"3FLpaOrgbgar","executionInfo":{"status":"ok","timestamp":1725160114371,"user_tz":-60,"elapsed":8,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}}},"outputs":[],"source":["num_classes = 2\n","\n","image_resize = 224\n","\n","batch_size_training = 100\n","batch_size_validation = 100"]},{"cell_type":"markdown","metadata":{"id":"mZ9oO3Nrbgas"},"source":["<a id='item34'></a>\n"]},{"cell_type":"markdown","metadata":{"id":"nVuwBspubgas"},"source":["## Construct ImageDataGenerator Instances\n"]},{"cell_type":"markdown","metadata":{"id":"bp0TiqgCbgas"},"source":["In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"]},{"cell_type":"code","execution_count":17,"metadata":{"tags":[],"id":"FukheT3Hbgas","executionInfo":{"status":"ok","timestamp":1725160114371,"user_tz":-60,"elapsed":7,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}}},"outputs":[],"source":["data_generator = ImageDataGenerator(\n","    preprocessing_function=preprocess_input,\n",")"]},{"cell_type":"markdown","metadata":{"id":"PQD0suGHbgat"},"source":["Next, we will use the *flow_from_directory* method to get the training images as follows:\n"]},{"cell_type":"code","execution_count":18,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"BEZ4_WcJbgat","executionInfo":{"status":"ok","timestamp":1725160115125,"user_tz":-60,"elapsed":760,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}},"outputId":"3c633e3e-4125-4411-f8cb-db0d226bd78b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 10001 images belonging to 2 classes.\n"]}],"source":["train_generator = data_generator.flow_from_directory(\n","    'concrete_data_week3/train',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_training,\n","    class_mode='categorical')"]},{"cell_type":"markdown","metadata":{"id":"DpC5jhpJbgat"},"source":["**Note**: in this lab, we will be using the full data-set of 30,000 images for training and validation.\n"]},{"cell_type":"markdown","metadata":{"id":"IDaJbiwZbgau"},"source":["**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"]},{"cell_type":"code","execution_count":19,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"nTyH351Dbgau","executionInfo":{"status":"ok","timestamp":1725160115126,"user_tz":-60,"elapsed":7,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}},"outputId":"45d39760-3387-49b0-f360-31c4b8092680"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5001 images belonging to 2 classes.\n"]}],"source":["## Type your answer here\n","validation_generator = data_generator.flow_from_directory(\n","    'concrete_data_week3/valid',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_validation,\n","    class_mode='categorical')\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OFU-QTM0bgav"},"source":["Double-click __here__ for the solution.\n","<!-- The correct answer is:\n","validation_generator = data_generator.flow_from_directory(\n","    'concrete_data_week3/valid',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_validation,\n","    class_mode='categorical')\n","-->\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dgw89lAGbgaw"},"source":["<a id='item35'></a>\n"]},{"cell_type":"markdown","metadata":{"id":"m_tg4GEAbgaw"},"source":["## Build, Compile and Fit Model\n"]},{"cell_type":"markdown","metadata":{"id":"Xlo8jkJlbgaw"},"source":["In this section, we will start building our model. We will use the Sequential model class from Keras.\n"]},{"cell_type":"code","execution_count":20,"metadata":{"tags":[],"id":"TGm0qa1Qbgax","executionInfo":{"status":"ok","timestamp":1725160115600,"user_tz":-60,"elapsed":478,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}}},"outputs":[],"source":["model = Sequential()"]},{"cell_type":"markdown","metadata":{"id":"LNB2naBsbgay"},"source":["Next, we will add the ResNet50 pre-trained model to our model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"]},{"cell_type":"code","execution_count":21,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"8D9INdOibgay","executionInfo":{"status":"ok","timestamp":1725160120332,"user_tz":-60,"elapsed":4734,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}},"outputId":"373f9e57-23f9-4cb5-d46d-284d06975fe1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94773248/94765736 [==============================] - 1s 0us/step\n","94781440/94765736 [==============================] - 1s 0us/step\n"]}],"source":["model.add(ResNet50(\n","    include_top=False,\n","    pooling='avg',\n","    weights='imagenet',\n","    ))"]},{"cell_type":"markdown","metadata":{"id":"XOv6yS8Fbgaz"},"source":["Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"]},{"cell_type":"code","execution_count":22,"metadata":{"tags":[],"id":"kPs9jEFebgaz","executionInfo":{"status":"ok","timestamp":1725160120333,"user_tz":-60,"elapsed":22,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}}},"outputs":[],"source":["model.add(Dense(num_classes, activation='softmax'))"]},{"cell_type":"markdown","metadata":{"id":"PPtanZxLbgaz"},"source":["You can access the model's layers using the *layers* attribute of our model object.\n"]},{"cell_type":"code","execution_count":23,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"GwfBhOTRbga0","executionInfo":{"status":"ok","timestamp":1725160120333,"user_tz":-60,"elapsed":21,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}},"outputId":"4cb48e75-21b4-4e4f-9534-2309c52b56c7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<keras.engine.functional.Functional at 0x78f2e77ff940>,\n"," <keras.layers.core.dense.Dense at 0x78f2e78158d0>]"]},"metadata":{},"execution_count":23}],"source":["model.layers"]},{"cell_type":"markdown","metadata":{"id":"8wqctmSqbga0"},"source":["You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"]},{"cell_type":"markdown","metadata":{"id":"NfGWsZYlbga0"},"source":["You can access the ResNet50 layers by running the following:\n"]},{"cell_type":"code","execution_count":24,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"EyFBzWwpbga1","executionInfo":{"status":"ok","timestamp":1725160120333,"user_tz":-60,"elapsed":20,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}},"outputId":"2b0f988f-d456-4638-942e-25a5fad5dde7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<keras.engine.input_layer.InputLayer at 0x78f311881b10>,\n"," <keras.layers.convolutional.ZeroPadding2D at 0x78f2e9a27a00>,\n"," <keras.layers.convolutional.Conv2D at 0x78f311881c90>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f311882c50>,\n"," <keras.layers.core.activation.Activation at 0x78f311882320>,\n"," <keras.layers.convolutional.ZeroPadding2D at 0x78f311882ec0>,\n"," <keras.layers.pooling.MaxPooling2D at 0x78f3118d8fd0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f3118d93c0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f3118dbbe0>,\n"," <keras.layers.core.activation.Activation at 0x78f3118d8f10>,\n"," <keras.layers.convolutional.Conv2D at 0x78f3118f4070>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f3118f6d70>,\n"," <keras.layers.core.activation.Activation at 0x78f3118f4a00>,\n"," <keras.layers.convolutional.Conv2D at 0x78f3118d9480>,\n"," <keras.layers.convolutional.Conv2D at 0x78f3118f51e0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f3118da7a0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f3118f6380>,\n"," <keras.layers.merge.Add at 0x78f3118f6080>,\n"," <keras.layers.core.activation.Activation at 0x78f311908a30>,\n"," <keras.layers.convolutional.Conv2D at 0x78f31190b970>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f31190b6a0>,\n"," <keras.layers.core.activation.Activation at 0x78f31190a860>,\n"," <keras.layers.convolutional.Conv2D at 0x78f311918d90>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f31190b100>,\n"," <keras.layers.core.activation.Activation at 0x78f311908d30>,\n"," <keras.layers.convolutional.Conv2D at 0x78f31191abf0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f31191b430>,\n"," <keras.layers.merge.Add at 0x78f31191a890>,\n"," <keras.layers.core.activation.Activation at 0x78f311920ca0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f311922680>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f311921e40>,\n"," <keras.layers.core.activation.Activation at 0x78f311921900>,\n"," <keras.layers.convolutional.Conv2D at 0x78f311923fd0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f311921f00>,\n"," <keras.layers.core.activation.Activation at 0x78f311923e50>,\n"," <keras.layers.convolutional.Conv2D at 0x78f31193a440>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f31193b1f0>,\n"," <keras.layers.merge.Add at 0x78f31193a0e0>,\n"," <keras.layers.core.activation.Activation at 0x78f31193bac0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f3118f49a0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f311908430>,\n"," <keras.layers.core.activation.Activation at 0x78f3118f6e00>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e76653c0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e7666d70>,\n"," <keras.layers.core.activation.Activation at 0x78f311923ac0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f31193b3a0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e76652d0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f3119208b0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e76753c0>,\n"," <keras.layers.merge.Add at 0x78f2e7664bb0>,\n"," <keras.layers.core.activation.Activation at 0x78f2e76768f0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e7677be0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e768c640>,\n"," <keras.layers.core.activation.Activation at 0x78f2e7676ec0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e768d570>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e768e8f0>,\n"," <keras.layers.core.activation.Activation at 0x78f2e76779d0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e768fb80>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e76a5030>,\n"," <keras.layers.merge.Add at 0x78f2e768ffd0>,\n"," <keras.layers.core.activation.Activation at 0x78f2e76a6560>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e76a74c0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e76a7d90>,\n"," <keras.layers.core.activation.Activation at 0x78f2e76a4970>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e76c14b0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e76c2c20>,\n"," <keras.layers.core.activation.Activation at 0x78f2e76c1150>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e76c38e0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e76c21a0>,\n"," <keras.layers.merge.Add at 0x78f2e76c0a30>,\n"," <keras.layers.core.activation.Activation at 0x78f2e76d8df0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e76db490>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e76db7c0>,\n"," <keras.layers.core.activation.Activation at 0x78f2e76da710>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e76f0c10>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e76d9900>,\n"," <keras.layers.core.activation.Activation at 0x78f2e76d9ea0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e76c05b0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e768d630>,\n"," <keras.layers.merge.Add at 0x78f2e768df90>,\n"," <keras.layers.core.activation.Activation at 0x78f2e7676680>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e76f1ba0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e76f3640>,\n"," <keras.layers.core.activation.Activation at 0x78f3118f6a70>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e76dc970>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e76de0e0>,\n"," <keras.layers.core.activation.Activation at 0x78f2e76dc4c0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f3119206d0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e76deec0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e7676710>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e76df760>,\n"," <keras.layers.merge.Add at 0x78f2e76deb60>,\n"," <keras.layers.core.activation.Activation at 0x78f2e7704eb0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e7706ce0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e7705330>,\n"," <keras.layers.core.activation.Activation at 0x78f2e7705bd0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e7707df0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e7704ca0>,\n"," <keras.layers.core.activation.Activation at 0x78f2e7707fd0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e7726740>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e77274f0>,\n"," <keras.layers.merge.Add at 0x78f2e77263e0>,\n"," <keras.layers.core.activation.Activation at 0x78f2e7725f30>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e77392d0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e7739b70>,\n"," <keras.layers.core.activation.Activation at 0x78f2e7739000>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e773b790>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e774c430>,\n"," <keras.layers.core.activation.Activation at 0x78f2e773b430>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e774d960>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e774eb30>,\n"," <keras.layers.merge.Add at 0x78f2e774fa60>,\n"," <keras.layers.core.activation.Activation at 0x78f2e774fac0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e773bf10>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e7727eb0>,\n"," <keras.layers.core.activation.Activation at 0x78f2e7726590>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e7667d60>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e76a4850>,\n"," <keras.layers.core.activation.Activation at 0x78f2e7704820>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e7769450>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e776a650>,\n"," <keras.layers.merge.Add at 0x78f311908370>,\n"," <keras.layers.core.activation.Activation at 0x78f2e776bbb0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e7770af0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e7768f10>,\n"," <keras.layers.core.activation.Activation at 0x78f2e77690f0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e7772aa0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e7773bb0>,\n"," <keras.layers.core.activation.Activation at 0x78f2e7772740>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e7785030>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e77867a0>,\n"," <keras.layers.merge.Add at 0x78f2e7784bb0>,\n"," <keras.layers.core.activation.Activation at 0x78f2e7785e40>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e7794ac0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e7787460>,\n"," <keras.layers.core.activation.Activation at 0x78f2e7787d00>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e7796200>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e7797970>,\n"," <keras.layers.core.activation.Activation at 0x78f2e7795ea0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e77ac7c0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e77ac460>,\n"," <keras.layers.merge.Add at 0x78f2e77944f0>,\n"," <keras.layers.core.activation.Activation at 0x78f2e77ae830>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e77c9240>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e77aec20>,\n"," <keras.layers.core.activation.Activation at 0x78f2e7706950>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e77cb670>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e77cbca0>,\n"," <keras.layers.core.activation.Activation at 0x78f2e77cb310>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e77af490>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e77ae290>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e77ad270>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e7784490>,\n"," <keras.layers.merge.Add at 0x78f2e77845b0>,\n"," <keras.layers.core.activation.Activation at 0x78f2e7771ab0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e76dd7e0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e77709a0>,\n"," <keras.layers.core.activation.Activation at 0x78f2e76dc550>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e77da080>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e77db850>,\n"," <keras.layers.core.activation.Activation at 0x78f2e77723b0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e77dbca0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e77db070>,\n"," <keras.layers.merge.Add at 0x78f2e77dbb20>,\n"," <keras.layers.core.activation.Activation at 0x78f2e77e6380>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e77e7cd0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e77e7a00>,\n"," <keras.layers.core.activation.Activation at 0x78f2e77e6fe0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e77ecfa0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e77ec790>,\n"," <keras.layers.core.activation.Activation at 0x78f2e77e7bb0>,\n"," <keras.layers.convolutional.Conv2D at 0x78f2e77efbe0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x78f2e77fcaf0>,\n"," <keras.layers.merge.Add at 0x78f2e77ef9a0>,\n"," <keras.layers.core.activation.Activation at 0x78f2e77ed4e0>,\n"," <keras.layers.pooling.GlobalAveragePooling2D at 0x78f2e76dc130>]"]},"metadata":{},"execution_count":24}],"source":["model.layers[0].layers"]},{"cell_type":"markdown","metadata":{"id":"XtPUSmRLbga1"},"source":["Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"]},{"cell_type":"code","execution_count":25,"metadata":{"tags":[],"id":"p5y7Qd1Ebga2","executionInfo":{"status":"ok","timestamp":1725160120333,"user_tz":-60,"elapsed":17,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}}},"outputs":[],"source":["model.layers[0].trainable = False"]},{"cell_type":"markdown","metadata":{"id":"j-Kb2T01bga2"},"source":["And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"]},{"cell_type":"code","execution_count":26,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"7FQ0aw4jbga2","executionInfo":{"status":"ok","timestamp":1725160120333,"user_tz":-60,"elapsed":17,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}},"outputId":"14ed8998-8adc-42db-ce80-94659b0e2e5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," resnet50 (Functional)       (None, 2048)              23587712  \n","                                                                 \n"," dense (Dense)               (None, 2)                 4098      \n","                                                                 \n","=================================================================\n","Total params: 23,591,810\n","Trainable params: 4,098\n","Non-trainable params: 23,587,712\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"ZHeH_7zXbga2"},"source":["Next we compile our model using the **adam** optimizer.\n"]},{"cell_type":"code","execution_count":27,"metadata":{"tags":[],"id":"RJlGuEpKbga2","executionInfo":{"status":"ok","timestamp":1725160120334,"user_tz":-60,"elapsed":10,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}}},"outputs":[],"source":["model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"03obODtobga3"},"source":["Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"]},{"cell_type":"code","execution_count":28,"metadata":{"tags":[],"id":"BwhizSlNbga3","executionInfo":{"status":"ok","timestamp":1725160120334,"user_tz":-60,"elapsed":10,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}}},"outputs":[],"source":["steps_per_epoch_training = len(train_generator)\n","steps_per_epoch_validation = len(validation_generator)\n","num_epochs = 2"]},{"cell_type":"markdown","metadata":{"id":"xi_8cmV4bga3"},"source":["Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"]},{"cell_type":"code","execution_count":29,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"hXLThnicbga4","executionInfo":{"status":"ok","timestamp":1725165979779,"user_tz":-60,"elapsed":5859455,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}},"outputId":"ec9dd68d-505a-41b6-d3e3-62676d3db54d"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-29-172e67583a70>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  fit_history = model.fit_generator(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","101/101 [==============================] - 2940s 29s/step - loss: 0.0596 - accuracy: 0.9824 - val_loss: 0.0170 - val_accuracy: 0.9958\n","Epoch 2/2\n","101/101 [==============================] - 2918s 29s/step - loss: 0.0100 - accuracy: 0.9983 - val_loss: 0.0113 - val_accuracy: 0.9972\n"]}],"source":["fit_history = model.fit_generator(\n","    train_generator,\n","    steps_per_epoch=steps_per_epoch_training,\n","    epochs=num_epochs,\n","    validation_data=validation_generator,\n","    validation_steps=steps_per_epoch_validation,\n","    verbose=1,\n",")"]},{"cell_type":"markdown","metadata":{"id":"zlnOVkWwbga4"},"source":["Now that the model is trained, you are ready to start using it to classify images.\n"]},{"cell_type":"markdown","metadata":{"id":"THkI1IRNbga4"},"source":["Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"]},{"cell_type":"code","execution_count":30,"metadata":{"tags":[],"id":"Sdg-zmgNbga5","executionInfo":{"status":"ok","timestamp":1725165980526,"user_tz":-60,"elapsed":756,"user":{"displayName":"Bolatan Ibrahim (Ehbraheem)","userId":"14532481274525663353"}}},"outputs":[],"source":["model.save('classifier_resnet_model.h5')"]},{"cell_type":"markdown","metadata":{"id":"aP6Ycmmfbga5"},"source":["Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"]},{"cell_type":"markdown","metadata":{"id":"MapvP6kxbga6"},"source":["### Thank you for completing this lab!\n","\n","This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"]},{"cell_type":"markdown","metadata":{"id":"kqvItncVbga6"},"source":["This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"]},{"cell_type":"markdown","metadata":{"id":"KzQFf3Y2bga6"},"source":["\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XG6Gf5W7bga6"},"source":["<hr>\n","\n","Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"prev_pub_hash":"cf2970a1d2c549fe86023eaa076d0ce4936c4275baf2cccfdad8fe6ce3a8a6c2","colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"716a9ad155a34b919f6be7b2013eb36e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d82595d2c6b8412b8f8fc436a027767c","IPY_MODEL_daa35ae1f892401da529c05d091497ab","IPY_MODEL_72464e2bdf974ff380c666d01c682d4a"],"layout":"IPY_MODEL_c477199dfc2142a1ad214f94d8a23304","tabbable":null,"tooltip":null}},"d82595d2c6b8412b8f8fc436a027767c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_8a88156e67bb435d873b8458729d037c","placeholder":"​","style":"IPY_MODEL_97ceef39cc1e40af9087eeea4cb2730e","tabbable":null,"tooltip":null,"value":"Downloading concrete_data_week3.zip: 100%"}},"daa35ae1f892401da529c05d091497ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_d73b246771c7405ba3fa31819815c789","max":97863179,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c141efd6ec449088c17a32eeb3d93c8","tabbable":null,"tooltip":null,"value":97863179}},"72464e2bdf974ff380c666d01c682d4a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_da5f6102fae542e8826ac2ad154bf48a","placeholder":"​","style":"IPY_MODEL_37a2e4704ccc4655bed0f8ce6ffd5b3d","tabbable":null,"tooltip":null,"value":" 97863179/97863179 [00:02&lt;00:00, 52063097.86it/s]"}},"c477199dfc2142a1ad214f94d8a23304":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a88156e67bb435d873b8458729d037c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97ceef39cc1e40af9087eeea4cb2730e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"d73b246771c7405ba3fa31819815c789":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c141efd6ec449088c17a32eeb3d93c8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"da5f6102fae542e8826ac2ad154bf48a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37a2e4704ccc4655bed0f8ce6ffd5b3d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"31e8b83893f747b39ae3dfc33c39b44c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7cf2196ca9a34b9ba8a43cd36c59ed97","IPY_MODEL_cc21ae39483b414cbe99b64f42973eed","IPY_MODEL_9099ea20b3b94a36a6aafa492651e6d7"],"layout":"IPY_MODEL_ffcf226cabbc411e911034cff2cadb14","tabbable":null,"tooltip":null}},"7cf2196ca9a34b9ba8a43cd36c59ed97":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_3bf2afd1d0b14cc78430814cea3fe9aa","placeholder":"​","style":"IPY_MODEL_a40944a791564640b940292edb76af8d","tabbable":null,"tooltip":null,"value":"Extracting concrete_data_week3.zip: 100%"}},"cc21ae39483b414cbe99b64f42973eed":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_1ae1830af8a14cf89cc1c25d5e8c0067","max":30036,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d90ef681d9fc4a9180797e0c4944a2e4","tabbable":null,"tooltip":null,"value":30036}},"9099ea20b3b94a36a6aafa492651e6d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_121a1dbe8a154f06b5dface1827f50c6","placeholder":"​","style":"IPY_MODEL_78b3fd205ed74a618d7fb95bcc8fa925","tabbable":null,"tooltip":null,"value":" 30036/30036 [00:10&lt;00:00, 1735.62it/s]"}},"ffcf226cabbc411e911034cff2cadb14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bf2afd1d0b14cc78430814cea3fe9aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a40944a791564640b940292edb76af8d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"1ae1830af8a14cf89cc1c25d5e8c0067":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d90ef681d9fc4a9180797e0c4944a2e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"121a1dbe8a154f06b5dface1827f50c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78b3fd205ed74a618d7fb95bcc8fa925":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}}}}},"nbformat":4,"nbformat_minor":0}